# -*- coding: utf-8 -*-
"""
This script is used to test a model generated by the 
training done in the Genetic Algorithm Script.
"""

import gym
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import os.path
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T


### Classes ###
class DQN(nn.Module):
    def __init__(self, img_width, img_height, num_actions):
        super().__init__()
        #self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)
        #self.fc2 = nn.Linear(in_features=24, out_features=32)
        #self.out = nn.Linear(in_features=32, out_features=num_actions)
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(in_features=448, out_features=256)
        self.out = nn.Linear(in_features=256, out_features=num_actions)
        
    def forward(self, t):
        #t = t.flatten(start_dim=1)
        #t = F.relu(self.fc1(t))
        #t = F.relu(self.fc2(t))
        #t = self.out(t)
        t = F.relu(self.conv1(t))
        t = F.relu(self.conv2(t))
        t = F.relu(self.conv3(t))
        t = t.flatten(start_dim=1)
        t = F.relu(self.fc1(t))
        t = self.out(t)
        t = F.softmax(t, dim=1)
        return t

class PacmanEnvManager():
    def __init__(self, device):
        self.device = device
        self.env = gym.make("MsPacman-v0").unwrapped
        self.env.reset()
        self.current_screen = None
        self.done = False

    def reset(self):
        self.env.reset()
        self.current_screen = None
    
    def getEnv(self):
        return self.env;

    def close(self):
        self.env.close()

    def render(self, mode="human"):
        return self.env.render(mode)        

    def num_actions_available(self):
        return self.env.action_space.n
    
    def get_actions_meaning(self):
        return self.env.get_action_meanings()
    
    def take_action(self, action):
        _, reward, self.done, _ = self.env.step(action.item())
        return torch.tensor([reward], device= self.device)
    
    def take_action2(self, action):
        _, reward, self.done, _ = self.env.step(action.item())
        return _, reward, self.done, _ 
    
    def take_sample_action(self):
        _, reward, self.done, _ = self.env.step(self.env.action_space.sample())
    
    def just_starting(self):
        return self.current_screen is None
    
    def get_state(self):
        if self.just_starting() or self.done:
            self.current_screen = self.get_processed_screen()
            black_screen = torch.zeros_like(self.current_screen)
            return black_screen
        else:
            s1 = self.current_screen
            s2 = self.get_processed_screen()
            self.current_screen = s2
            return s2 - s1
    
    def get_screen_height(self):
        screen = self.get_processed_screen()
        return screen.shape[2]
    
    def get_screen_width(self):
        screen = self.get_processed_screen()
        return screen.shape[3]
    
    def get_observation_space(self):
        return self.env.observation_space.shape
    
    def get_processed_screen(self):
        screen = self.render('rgb_array').transpose((2, 0, 1))
        screen = self.crop_screen(screen)
        return self.transform_screen_data(screen)
    
    def crop_screen(self, screen):
        screen_height = screen.shape[1]
        
        top = int(screen_height * 0.4)
        bottom = int(screen_height * 0.8)
        screen = screen[:, top:bottom, :]
        return screen
    
    def transform_screen_data(self, screen):
        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
        screen = torch.from_numpy(screen)
        
        resize = T.Compose([
            T.ToPILImage()
            , T.Resize((40, 90))
            , T.ToTensor()
            ])
        
        return resize(screen).unsqueeze(0).to(self.device)

### Functions ###
def evaluatePlots(rewards_list , num):
    plt.title("Rewards per episode")
    plt.xlabel("Episodes")
    plt.ylabel("Rewards")
    plt.plot(range(1,num+1), rewards_list)
    plt.show()
    
### Main ###
if __name__== "__main__":
    
    model_path = "Trained_DQN_GA_Model.pth"
    
    exists = os.path.isfile(model_path)    
    if exists is None:
        print("""
          Model file "Trained_DQN_GA_Model.pth" does not exist.
          Please run GA_Train.py script first to get a trained model
          Please note, running the train script can take up to 24 hours or 
          more depending on PC specs.
          """)
    else:    
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        em = PacmanEnvManager(device)
        model = DQN(em.get_screen_height(), em.get_screen_width(), em.num_actions_available()).to(device)
        model.load_state_dict(torch.load(model_path))
        model.eval()

        numRound = 100 # Adjust this param to increase or decrease number of rounds tested

        idx = 0
        rewardScore = 0
        rewards_list = []
        

        print("Testing has begun. %d episodes will be executed."% numRound)
        while True:
            em.render()
            if idx >= numRound: 
                print("Testing has completed %d episodes."% numRound)
                break
            
            state = em.get_state()
            action = model(state)
            reward = em.take_action(action.argmax(dim=1).to(device))
            rewardScore += reward.item()
        
            if em.done:
               rewards_list.append(rewardScore)
               idx += 1
               rewardScore = 0 
               em.reset()
        em.close()
        evaluatePlots(rewards_list, numRound)